{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "!pip install numpy pandas matplotlib sktime tqdm tensorflow scikit-image openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sktime.datasets import load_from_tsfile_to_dataframe\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/Users/amanmaurya/Desktop/TSC/Datasets\" # Change the path to folder containing datasets.\n",
    "dataset_name = \"BasicMotions\" # Change this to the dataset name\n",
    "train_file = os.path.join(dataset_dir, dataset_name, f\"{dataset_name}_TRAIN.ts\")\n",
    "test_file = os.path.join(dataset_dir, dataset_name, f\"{dataset_name}_TEST.ts\")\n",
    "\n",
    "if not os.path.exists(train_file) or not os.path.exists(test_file):\n",
    "    raise FileNotFoundError(f\"Dataset files not found at {dataset_dir}. Check dataset path and structure.\")\n",
    "\n",
    "X_train, y_train = load_from_tsfile_to_dataframe(train_file)\n",
    "X_test, y_test = load_from_tsfile_to_dataframe(test_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characteristics of Loaded Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = {\n",
    "    \"Dataset\": [],\n",
    "    \"Train size\": [],\n",
    "    \"Test size\": [],\n",
    "    \"Length\": [],\n",
    "    \"Number of classes\": [],\n",
    "    \"Number of dimensions\": []\n",
    "}\n",
    "\n",
    "# Gather dataset properties\n",
    "train_size = X_train.shape[0]\n",
    "test_size = X_test.shape[0]\n",
    "sample_lengths = [len(X_train.iloc[i, 0]) for i in range(train_size)]\n",
    "length = sample_lengths[0]\n",
    "num_classes = len(np.unique(y_train))\n",
    "num_dimensions = X_train.shape[1]\n",
    "\n",
    "# Append to summary table\n",
    "summary_data[\"Dataset\"].append(dataset_name)\n",
    "summary_data[\"Train size\"].append(train_size)\n",
    "summary_data[\"Test size\"].append(test_size)\n",
    "summary_data[\"Length\"].append(length)\n",
    "summary_data[\"Number of classes\"].append(num_classes)\n",
    "summary_data[\"Number of dimensions\"].append(num_dimensions)\n",
    "\n",
    "# Convert summary to DataFrame and display\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory for saving generated images\n",
    "output_dir = \"Generated_Images/BasicMotions\" # Change the name to the name of currently working dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create directories\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# Function to save an image for a given instance and dimension\n",
    "def save_image(img, variant, dataset_type, label, instance_idx, dimension_idx):\n",
    "    path = os.path.join(output_dir, variant, dataset_type, f\"Class_{label}\")\n",
    "    create_dir(path)\n",
    "    file_name = os.path.join(path, f\"instance_{instance_idx}_dim_{dimension_idx}.png\")\n",
    "    plt.imsave(file_name, img, cmap='gray')\n",
    "\n",
    "# Function to perform scaling to a target range\n",
    "def scale_to_range(data, target_min, target_max):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    return target_min + (data - min_val) * (target_max - target_min) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(X, y, dataset_type, method):\n",
    "    for instance_idx, (instance, label) in enumerate(zip(X.iterrows(), y)):\n",
    "        instance_idx, features = instance  # features is a pandas Series where each cell is a pandas.Series\n",
    "        for dim_idx, ts_values in enumerate(features):  # Iterate over each feature (dimension)\n",
    "            ts_values = np.array(ts_values)  # Convert pandas.Series to numpy array\n",
    "            \n",
    "            if method == \"Variant 1\":\n",
    "                # Val/ValChng Transformation\n",
    "                first_diff = np.diff(ts_values)\n",
    "                scaled_values = scale_to_range(ts_values[1:], 0, 127)\n",
    "                scaled_diff = scale_to_range(first_diff, 0, 127)\n",
    "                img = np.zeros((128,128))\n",
    "                for i, j in zip(scaled_values.astype(int), scaled_diff.astype(int)):\n",
    "                    img[i, j] = min(img[i, j] + 1, 255)\n",
    "                save_image(img, method, dataset_type, label, instance_idx, dim_idx)\n",
    "\n",
    "            elif method == \"Variant 2\":\n",
    "                # ValChng/ChngValChng Transformation\n",
    "                first_diff = np.diff(ts_values)\n",
    "                second_diff = np.diff(first_diff)\n",
    "                scaled_first_diff = scale_to_range(first_diff[1:], 0, 127)\n",
    "                scaled_second_diff = scale_to_range(second_diff, 0, 127)\n",
    "                img = np.zeros((128,128))\n",
    "                for i, j in zip(scaled_first_diff.astype(int), scaled_second_diff.astype(int)):\n",
    "                    img[i, j] = min(img[i, j] + 1, 255)\n",
    "                save_image(img, method, dataset_type, label, instance_idx, dim_idx)\n",
    "\n",
    "            elif method == \"Variant 3\":\n",
    "                # Values x Values Transformation\n",
    "                scaled_values = scale_to_range(ts_values, 0, 1)\n",
    "                img = np.outer(scaled_values, scaled_values)\n",
    "                save_image(img, method, dataset_type, label, instance_idx, dim_idx)\n",
    "\n",
    "            elif method == \"Variant 4\":\n",
    "                # ReplVal Transformation\n",
    "                scaled_values = scale_to_range(ts_values, 0, 1)\n",
    "                img = np.tile(scaled_values, (len(ts_values), 1))\n",
    "                save_image(img, method, dataset_type, label, instance_idx, dim_idx)\n",
    "\n",
    "            elif method == \"Variant 5\":\n",
    "                # ReplValChng Transformation\n",
    "                first_diff = np.diff(ts_values)\n",
    "                scaled_diff = scale_to_range(first_diff, 0, 1)\n",
    "                img = np.tile(scaled_diff, (len(first_diff), 1))\n",
    "                save_image(img, method, dataset_type, label, instance_idx, dim_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main loop to generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"Variant 1\", \"Variant 2\", \"Variant 3\", \"Variant 4\", \"Variant 5\"]\n",
    "for method in methods:\n",
    "    for dataset_type, (X, y) in zip([\"Train\", \"Test\"], [(X_train, y_train), (X_test, y_test)]):\n",
    "        generate_images(X, y, dataset_type, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Images from Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size for all variants\n",
    "IMG_SIZE = (128,128)  # You can adjust this size\n",
    "\n",
    "# Directory containing your images\n",
    "image_dir = '/Users/amanmaurya/Desktop/TIme-Series-Classification-Via-Image-Transformations/Generated_Images/BasicMotions' # Change to the path of currently working generated images.\n",
    "\n",
    "# Helper function to load and preprocess images for each variant\n",
    "def load_images(image_dir, method, dataset_type, IMG_SIZE=IMG_SIZE):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    method_dir = os.path.join(image_dir, method, dataset_type)\n",
    "    \n",
    "    # Loop through classes and load images\n",
    "    for class_dir in os.listdir(method_dir):\n",
    "        class_path = os.path.join(method_dir, class_dir)\n",
    "        for image_file in os.listdir(class_path):\n",
    "            if image_file.endswith('.png'):  # Assuming the images are in PNG format\n",
    "                img_path = os.path.join(class_path, image_file)\n",
    "                img = image.load_img(img_path, target_size=IMG_SIZE)\n",
    "                img_array = image.img_to_array(img)  # Convert to numpy array\n",
    "                img_array = img_array / 255.0  # Normalize the image\n",
    "                \n",
    "                images.append(img_array)\n",
    "                labels.append(class_dir)  # Use class directory name as label\n",
    "    \n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to encode labels as integers\n",
    "def encode_labels(labels):\n",
    "    le = LabelEncoder()\n",
    "    return le.fit_transform(labels), le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN model as per the specifications\n",
    "def build_cnn_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # First Convolutional Block\n",
    "    model.add(layers.Conv2D(64, (1, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((1, 2)))\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    model.add(layers.Conv2D(64, (1, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((1, 2)))\n",
    "    \n",
    "    # Third Convolutional Block \n",
    "    model.add(layers.Conv2D(128, (1, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((1, 2)))\n",
    "    \n",
    "    # Flatten and Dense Layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    \n",
    "    # Dropout Layer\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    \n",
    "    # Output Layer with Softmax\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to train and test the CNN for each variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_variant(image_dir, method, IMG_SIZE=IMG_SIZE):\n",
    "    print(f\"\\nProcessing {method}...\")\n",
    "\n",
    "    # Load Train data\n",
    "    print(\"Loading Train data...\")\n",
    "    train_images, train_labels = load_images(image_dir, method, \"Train\", IMG_SIZE)\n",
    "    train_labels, label_encoder = encode_labels(train_labels)\n",
    "    \n",
    "    # Load Test data\n",
    "    print(\"Loading Test data...\")\n",
    "    test_images, test_labels = load_images(image_dir, method, \"Test\", IMG_SIZE)\n",
    "    test_labels = label_encoder.transform(test_labels)  # Transform test labels using the same encoder\n",
    "    \n",
    "    # Build the CNN model\n",
    "    input_shape = (IMG_SIZE[0], IMG_SIZE[1], 3)  # 3 channels for RGB images\n",
    "    num_classes = len(label_encoder.classes_)  # Number of classes based on label encoder\n",
    "    model = build_cnn_model(input_shape, num_classes)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "    \n",
    "    # Evaluate the model on the Test data\n",
    "    print(\"Testing the model...\")\n",
    "    test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "    print(f\"Test Accuracy for {method}: {test_accuracy * 100:.2f}%\")\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test the Model on all Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each variant\n",
    "methods = [\"Variant 1\", \"Variant 2\", \"Variant 3\", \"Variant 4\", \"Variant 5\"]\n",
    "\n",
    "# Train and test on Train and Test datasets for each variant\n",
    "results = []\n",
    "for method in methods:\n",
    "    test_accuracy = train_and_test_variant(image_dir, method, IMG_SIZE)\n",
    "    results.append({\"Variant\": method, \"Test Accuracy (%)\": test_accuracy * 100})\n",
    "\n",
    "# Save results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_excel(\"BasicMotions_CNN_Results.xlsx\", index=False) # Change the name to currently working dataset\n",
    "\n",
    "print(\"\\nResults saved to 'BasicMotions_CNN_Results.xlsx'\") # Change the name to currently working dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries for Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16, VGG19, ResNet50, EfficientNetB3, MobileNetV2\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to use Transfer Learning for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transfer_learning(image_dir, method, model_name, base_model_func, IMG_SIZE=IMG_SIZE):\n",
    "    \"\"\"\n",
    "    Perform transfer learning using a pre-trained model.\n",
    "    \"\"\"\n",
    "    print(f\"\\nApplying Transfer Learning on {method} using {model_name}...\")\n",
    "\n",
    "    # Load Train data\n",
    "    print(\"Loading Train data...\")\n",
    "    train_images, train_labels = load_images(image_dir, method, \"Train\", IMG_SIZE)\n",
    "    train_labels, label_encoder = encode_labels(train_labels)\n",
    "\n",
    "    # Load Test data\n",
    "    print(\"Loading Test data...\")\n",
    "    test_images, test_labels = load_images(image_dir, method, \"Test\", IMG_SIZE)\n",
    "    test_labels = label_encoder.transform(test_labels)  # Transform test labels using the same encoder\n",
    "\n",
    "    # Load pre-trained model\n",
    "    base_model = base_model_func(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "    # Freeze the base model layers\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Build the transfer learning model\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(len(label_encoder.classes_), activation='softmax')  # Output layer\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "    # Evaluate the model on the Test data\n",
    "    print(\"Testing the model...\")\n",
    "    test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "    print(f\"Transfer Learning Test Accuracy for {method} using {model_name}: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "    return test_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Transfer Learning on all variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and their corresponding functions\n",
    "models_info = [\n",
    "    (\"VGG16\", VGG16),\n",
    "    (\"VGG19\", VGG19),\n",
    "    (\"ResNet50\", ResNet50),\n",
    "    (\"EfficientNetB3\", EfficientNetB3),\n",
    "    (\"MobileNetV2\", MobileNetV2)\n",
    "]\n",
    "\n",
    "# Process each variant with Transfer Learning for all models\n",
    "all_results = []\n",
    "\n",
    "for model_name, base_model_func in models_info:\n",
    "    print(f\"\\nProcessing {model_name}...\")\n",
    "    transfer_results = []\n",
    "    for method in methods:\n",
    "        test_accuracy = transfer_learning(image_dir, method, model_name, base_model_func, IMG_SIZE)\n",
    "        transfer_results.append({\"Model\": model_name, \"Variant\": method, \"Test Accuracy (%)\": test_accuracy * 100})\n",
    "    \n",
    "    all_results.extend(transfer_results)\n",
    "\n",
    "# Save transfer learning results to an Excel file\n",
    "all_results_df = pd.DataFrame(all_results)\n",
    "all_results_df.to_excel(\"BasicMotions_Transfer_Learning.xlsx\", index=False) # Change the name to currently working dataset\n",
    "\n",
    "print(\"\\nTransfer learning results saved to 'BasicMotions_Transfer_Learning.xlsx'\") # Change the name to currently working dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
